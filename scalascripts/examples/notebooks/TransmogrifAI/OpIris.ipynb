{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Op Iris Sample\n",
    "\n",
    "Derived from https://github.com/salesforce/TransmogrifAI/tree/master/helloworld/notebooks\n",
    "\n",
    "The following code illustrates how TransmogrifAI can be used to do classify multiple classes over the Iris dataset.\n",
    "\n",
    "First we need to load the transmogrifai and Spark Mllib jars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                                       \u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.3.3`\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.3.3`\n",
    "import $ivy.`sh.almond::almond-spark:0.5.0`\n",
    "import $ivy.`com.salesforce.transmogrifai::transmogrifai-core:0.5.1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want avoid too extensive logging and long outputs in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org.apache.spark\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"breeze\").setLevel(Level.WARN)\n",
    "Logger.getLogger(\"com.salesforce.op\").setLevel(Level.WARN)\n",
    "\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define features\n",
    "\n",
    "Let us first define the case Class which describes the schema for the data.\n",
    "\n",
    "For now, we also need a few workarounds here for issues caused by the class wrapping required for serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mIris\u001b[39m\n",
       "\u001b[36mirisTypeTag\u001b[39m: \u001b[32mreflect\u001b[39m.\u001b[32mruntime\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32muniverse\u001b[39m.\u001b[32mWeakTypeTag\u001b[39m[\u001b[32mIris\u001b[39m] = TypeTag[Helper.this.Iris]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Needed for now for case classes defined within Ammonite. Won't be necessary in future versions of Spark.\n",
    "// See https://github.com/alexarchambault/ammonite-spark/issues/19 and https://github.com/apache/spark/pull/23607\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this)\n",
    "case class Iris(\n",
    "  sepalLength: Double,\n",
    "  sepalWidth: Double,\n",
    "  petalLength: Double,\n",
    "  petalWidth: Double,\n",
    "  irisClass: String\n",
    ")\n",
    "// Required to make sure the String representation of the case class doesn't change in later cells.\n",
    "implicit val irisTypeTag = scala.reflect.runtime.universe.weakTypeTag[Iris]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "\n",
    "We then define the set of raw features that we would like to extract from the data. The raw features are defined using [FeatureBuilders](https://docs.transmogrif.ai/Developer-Guide#featurebuilders), and are strongly typed. TransmogrifAI supports the following basic feature types: `Text`, `Numeric`, `Vector`, `List` , `Set`, `Map`. \n",
    "In addition it supports many specific feature types which extend these base types: Email extends Text; Integral, Real and Binary extend Numeric; Currency and Percentage extend Real. For a complete view of the types supported see the Type Hierarchy and Automatic Feature Engineering section in the Documentation.\n",
    "\n",
    "Basic `FeatureBuilders` will be created for you if you use the TransmogrifAI CLI to bootstrap your project as described here. However, it is often useful to edit this code to customize feature generation and take full advantage of the Feature types available (selecting the appropriate type will improve automatic feature engineering steps).\n",
    "\n",
    "When defining raw features, specify the extract logic to be applied to the raw data, and also annotate the features as either predictor or response variables via the FeatureBuilders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.features.FeatureBuilder\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.features.types._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36msepalLength\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeature\u001b[39m[\u001b[32mReal\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"sepalLength\"\u001b[39m,\n",
       "  false,\n",
       "  FeatureGeneratorStage_000000000001,\n",
       "  \u001b[33mList\u001b[39m(),\n",
       "  \u001b[32m\"Real_000000000001\"\u001b[39m,\n",
       "  \u001b[33mList\u001b[39m()\n",
       ")\n",
       "\u001b[36msepalWidth\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeature\u001b[39m[\u001b[32mReal\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"sepalWidth\"\u001b[39m,\n",
       "  false,\n",
       "  FeatureGeneratorStage_000000000002,\n",
       "  \u001b[33mList\u001b[39m(),\n",
       "  \u001b[32m\"Real_000000000002\"\u001b[39m,\n",
       "  \u001b[33mList\u001b[39m()\n",
       ")\n",
       "\u001b[36mpetalLength\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeature\u001b[39m[\u001b[32mReal\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"petalLength\"\u001b[39m,\n",
       "  false,\n",
       "  FeatureGeneratorStage_000000000003,\n",
       "  \u001b[33mList\u001b[39m(),\n",
       "  \u001b[32m\"Real_000000000003\"\u001b[39m,\n",
       "  \u001b[33mList\u001b[39m()\n",
       ")\n",
       "\u001b[36mpetalWidth\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeature\u001b[39m[\u001b[32mReal\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"petalWidth\"\u001b[39m,\n",
       "  false,\n",
       "  FeatureGeneratorStage_000000000004,\n",
       "  \u001b[33mList\u001b[39m(),\n",
       "  \u001b[32m\"Real_000000000004\"\u001b[39m,\n",
       "  \u001b[33mList\u001b[39m()\n",
       ")\n",
       "\u001b[36mirisClass\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeature\u001b[39m[\u001b[32mText\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"irisClass\"\u001b[39m,\n",
       "  true,\n",
       "  FeatureGeneratorStage_000000000005,\n",
       "  \u001b[33mList\u001b[39m(),\n",
       "  \u001b[32m\"Text_000000000005\"\u001b[39m,\n",
       "  \u001b[33mList\u001b[39m()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.salesforce.op.features.FeatureBuilder\n",
    "import com.salesforce.op.features.types._\n",
    "\n",
    "val sepalLength = FeatureBuilder.Real[Iris].extract(_.sepalLength.toReal).asPredictor\n",
    "val sepalWidth = FeatureBuilder.Real[Iris].extract(_.sepalWidth.toReal).asPredictor\n",
    "val petalLength = FeatureBuilder.Real[Iris].extract(_.petalLength.toReal).asPredictor\n",
    "val petalWidth = FeatureBuilder.Real[Iris].extract(_.petalWidth.toReal).asPredictor\n",
    "val irisClass = FeatureBuilder.Text[Iris].extract(_.irisClass.toText).asResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Getting spark JARs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log4j:WARN No appenders could be found for logger (org.eclipse.jetty.util.log).\n",
      "log4j:WARN Please initialize the log4j system properly.\n",
      "log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "19/04/04 22:59:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"http://sorens-mbp.fritz.box:4040\">Spark UI</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@468e9d5e"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "implicit val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .progress(false)\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.evaluators.Evaluators\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.readers.DataReaders\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.stages.impl.classification.MultiClassificationModelSelector\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.stages.impl.tuning.DataCutter\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.Encoders\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.salesforce.op._\n",
    "import com.salesforce.op.evaluators.Evaluators\n",
    "import com.salesforce.op.readers.DataReaders\n",
    "import com.salesforce.op.stages.impl.classification.MultiClassificationModelSelector\n",
    "import com.salesforce.op.stages.impl.tuning.DataCutter\n",
    "import org.apache.spark.sql.Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to encode the case class using `org.apache.spark.sql.Encoders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mirisEncoder\u001b[39m: \u001b[32mEncoder\u001b[39m[\u001b[32mIris\u001b[39m] = \u001b[33mExpressionEncoder\u001b[39m(\n",
       "  \u001b[33mStructType\u001b[39m(\n",
       "    \u001b[33mStructField\u001b[39m(\u001b[32m\"sepalLength\"\u001b[39m, DoubleType, false, {}),\n",
       "    \u001b[33mStructField\u001b[39m(\u001b[32m\"sepalWidth\"\u001b[39m, DoubleType, false, {}),\n",
       "    \u001b[33mStructField\u001b[39m(\u001b[32m\"petalLength\"\u001b[39m, DoubleType, false, {}),\n",
       "    \u001b[33mStructField\u001b[39m(\u001b[32m\"petalWidth\"\u001b[39m, DoubleType, false, {}),\n",
       "    \u001b[33mStructField\u001b[39m(\u001b[32m\"irisClass\"\u001b[39m, StringType, true, {})\n",
       "  ),\n",
       "  false,\n",
       "  \u001b[33mList\u001b[39m(\n",
       "    \u001b[33mAlias\u001b[39m(\n",
       "      \u001b[33mInvoke\u001b[39m(\n",
       "        \u001b[33mAssertNotNull\u001b[39m(\n",
       "          \u001b[33mAssertNotNull\u001b[39m(\n",
       "..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val irisEncoder = Encoders.product[Iris]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a DataRead which will load csv and map to schema of type Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mirisReader\u001b[39m: \u001b[32mreaders\u001b[39m.\u001b[32mCSVProductReader\u001b[39m[\u001b[32mIris\u001b[39m] = com.salesforce.op.readers.CSVProductReader@3bac5612"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val irisReader = DataReaders.Simple.csvCase[Iris]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Engineering\n",
    "See [Creating Shortcuts for Transformers and Estimators](https://docs.transmogrif.ai/en/stable/developer-guide#creating-shortcuts-for-transformers-and-estimators) for more documentation on how shortcuts for stages can be created. We now define a Feature of type `Vector`, that is a vector representation of all the features we would like to use as predictors in our workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mlabels\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeatureLike\u001b[39m[\u001b[32mRealNN\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"irisClass_1-stagesApplied_RealNN_000000000006\"\u001b[39m,\n",
       "  true,\n",
       "  OpStringIndexerNoFilter_000000000006,\n",
       "  \u001b[33mWrappedArray\u001b[39m(\n",
       "    \u001b[33mFeature\u001b[39m(\n",
       "      \u001b[32m\"irisClass\"\u001b[39m,\n",
       "      true,\n",
       "      FeatureGeneratorStage_000000000005,\n",
       "      \u001b[33mList\u001b[39m(),\n",
       "      \u001b[32m\"Text_000000000005\"\u001b[39m,\n",
       "      \u001b[33mList\u001b[39m()\n",
       "    )\n",
       "  ),\n",
       "...\n",
       "\u001b[36mfeatures\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeatureLike\u001b[39m[\u001b[32mOPVector\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"petalLength-petalWidth-sepalLength-sepalWidth_2-stagesApplied_OPVector_000000000008\"\u001b[39m,\n",
       "  false,\n",
       "  VectorsCombiner_000000000008,\n",
       "  \u001b[33mWrappedArray\u001b[39m(\n",
       "    \u001b[33mFeature\u001b[39m(\n",
       "      \u001b[32m\"petalLength-petalWidth-sepalLength-sepalWidth_1-stagesApplied_OPVector_000000000007\"\u001b[39m,\n",
       "      false,\n",
       "      RealVectorizer_000000000007,\n",
       "      \u001b[33mWrappedArray\u001b[39m(\n",
       "        \u001b[33mFeature\u001b[39m(\n",
       "          \u001b[32m\"sepalLength\"\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val labels = irisClass.indexed()\n",
    "val features = Seq(sepalLength, sepalWidth, petalLength, petalWidth).transmogrify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create a DataCutter : Creates instance that will split data into training and test set filtering out any labels that don't meet the minimum fraction cutoff or fall in the top N labels specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mrandomSeed\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m42L\u001b[39m\n",
       "\u001b[36mcutter\u001b[39m: \u001b[32mDataCutter\u001b[39m = DataCutter_000000000009"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val randomSeed = 42L\n",
    "val cutter = DataCutter(reserveTestFraction = 0.2, seed = randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a MultiClassModelSelector and specify splitter created above. Then set the input - labels and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19/04/04 23:00:03 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "19/04/04 23:00:03 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mprediction\u001b[39m: \u001b[32mcom\u001b[39m.\u001b[32msalesforce\u001b[39m.\u001b[32mop\u001b[39m.\u001b[32mfeatures\u001b[39m.\u001b[32mFeatureLike\u001b[39m[\u001b[32mPrediction\u001b[39m] = \u001b[33mFeature\u001b[39m(\n",
       "  \u001b[32m\"irisClass-petalLength-petalWidth-sepalLength-sepalWidth_4-stagesApplied_Prediction_000000000011\"\u001b[39m,\n",
       "  true,\n",
       "  ModelSelector_000000000011,\n",
       "  \u001b[33mWrappedArray\u001b[39m(\n",
       "    \u001b[33mFeature\u001b[39m(\n",
       "      \u001b[32m\"irisClass_1-stagesApplied_RealNN_000000000006\"\u001b[39m,\n",
       "      true,\n",
       "      OpStringIndexerNoFilter_000000000006,\n",
       "      \u001b[33mWrappedArray\u001b[39m(\n",
       "        \u001b[33mFeature\u001b[39m(\n",
       "          \u001b[32m\"irisClass\"\u001b[39m,\n",
       "          true,\n",
       "..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val prediction = MultiClassificationModelSelector\n",
    "    .withCrossValidation(splitter = Option(cutter), seed = randomSeed)\n",
    "    .setInput(labels, features).getOutput()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mevaluator\u001b[39m: \u001b[32mevaluators\u001b[39m.\u001b[32mOpMultiClassificationEvaluator\u001b[39m = OpMultiClassificationEvaluator_000000000012"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = Evaluators.MultiClassification.f1().setLabelCol(labels).setPredictionCol(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we’ve done so far has been purely at the level of definitions. We have defined how we would like to extract our raw features from data of type `Iris`, and we have defined how we would like to manipulate them. In order to actually manifest the data described by these features, we need to add them to a workflow and attach a data source to the workflow.\n",
    "\n",
    "Please note the `trainFilePath` is the derived path from folder where host folder is mounted as a volume (/home/beakerx/helloworld) in this case. This can be changed as well depending on the location and volume director you are mounting the data from. You can also create a new DataReader with a new path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mcom.salesforce.op.readers.DataReaders\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mtrainFilePath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"datasets/IrisDataset/iris.data\"\u001b[39m\n",
       "\u001b[36mtrainDataReader\u001b[39m: \u001b[32mreaders\u001b[39m.\u001b[32mCSVProductReader\u001b[39m[\u001b[32mIris\u001b[39m] = com.salesforce.op.readers.CSVProductReader@39c7c465"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import com.salesforce.op.readers.DataReaders\n",
    "\n",
    "val trainFilePath = \"datasets/IrisDataset/iris.data\"\n",
    "// Define a way to read data into our Passenger class from our CSV file\n",
    "val trainDataReader = DataReaders.Simple.csvCase[Iris](\n",
    "  path = Option(trainFilePath)\n",
    "  //key = _.id.toString\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Workflow for TransmogrifAI. Takes the final features that the user wants to generate as inputs and constructs the full DAG needed to generate them from those features lineage. Then fits any estimators in the pipeline dag to create a sequence of transformations that are saved in a workflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mworkflow\u001b[39m: \u001b[32mOpWorkflow\u001b[39m = com.salesforce.op.OpWorkflow@73e86832"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val workflow = new OpWorkflow().setResultFeatures(prediction, labels).setReader(trainDataReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we now call ‘train’ on this workflow, it automatically computes and executes the entire DAG of Stages needed to compute the features  fitting all the estimators on the training data in the process. Calling score on the fitted workflow then transforms the underlying training data to produce a DataFrame with the all the features manifested. The score method can optionally be passed an evaluator that produces metrics.\n",
    "`workflow.train()` methods fits all of the estimators in the pipeline and return a pipeline model of only transformers. Uses data loaded as specified by the data reader to generate the initial data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      "Evaluated OpRandomForestClassifier, OpLogisticRegression models using Cross Validation and error metric.\n",
      "Evaluated 18 OpRandomForestClassifier models with error metric between [0.06780584574255233, 0.7332635181732267].\n",
      "Evaluated 8 OpLogisticRegression models with error metric between [0.057780559032123494, 0.2260946931964275].\n",
      "+--------------------------------------------------------+\n",
      "|         Selected Model - OpLogisticRegression          |\n",
      "+--------------------------------------------------------+\n",
      "| Model Param      | Value                               |\n",
      "+------------------+-------------------------------------+\n",
      "| aggregationDepth | 2                                   |\n",
      "| elasticNetParam  | 0.5                                 |\n",
      "| family           | auto                                |\n",
      "| fitIntercept     | true                                |\n",
      "| maxIter          | 50                                  |\n",
      "| modelType        | OpLogisticRegression                |\n",
      "| name             | OpLogisticRegression_00000000000c_3 |\n",
      "| regParam         | 0.01                                |\n",
      "| standardization  | true                                |\n",
      "| tol              | 1.0E-6                              |\n",
      "| uid              | OpLogisticRegression_00000000000c   |\n",
      "+------------------+-------------------------------------+\n",
      "+-------------------------------------------------------+\n",
      "|               Model Evaluation Metrics                |\n",
      "+-------------------------------------------------------+\n",
      "| Metric Name | Training Set Value | Hold Out Set Value |\n",
      "+-------------+--------------------+--------------------+\n",
      "| error       | 0.0461538461538461 | 0.0                |\n",
      "| f1          | 0.9542123135981102 | 1.0                |\n",
      "| precision   | 0.9545787545787545 | 1.0                |\n",
      "| recall      | 0.9538461538461539 | 1.0                |\n",
      "+-------------+--------------------+--------------------+\n",
      "+-----------------------------------------------------------+\n",
      "|                    Top Model Insights                     |\n",
      "+-----------------------------------------------------------+\n",
      "| Top Positive Correlations       |       Correlation Value |\n",
      "+---------------------------------+-------------------------+\n",
      "| sepalWidth(sepalWidth = null)   | -1.7976931348623157E308 |\n",
      "| sepalWidth                      | -1.7976931348623157E308 |\n",
      "| sepalLength(sepalLength = null) | -1.7976931348623157E308 |\n",
      "| sepalLength                     | -1.7976931348623157E308 |\n",
      "| petalLength(petalLength = null) | -1.7976931348623157E308 |\n",
      "| petalLength                     | -1.7976931348623157E308 |\n",
      "| petalWidth(petalWidth = null)   | -1.7976931348623157E308 |\n",
      "| petalWidth                      | -1.7976931348623157E308 |\n",
      "+---------------------------------+-------------------------+\n",
      "+----------------------------------------------------------+\n",
      "| Top Negative Correlations       |      Correlation Value |\n",
      "+---------------------------------+------------------------+\n",
      "| petalWidth                      | 1.7976931348623157E308 |\n",
      "| petalWidth(petalWidth = null)   | 1.7976931348623157E308 |\n",
      "| petalLength                     | 1.7976931348623157E308 |\n",
      "| petalLength(petalLength = null) | 1.7976931348623157E308 |\n",
      "| sepalLength                     | 1.7976931348623157E308 |\n",
      "| sepalLength(sepalLength = null) | 1.7976931348623157E308 |\n",
      "| sepalWidth                      | 1.7976931348623157E308 |\n",
      "| sepalWidth(sepalWidth = null)   | 1.7976931348623157E308 |\n",
      "+---------------------------------+------------------------+\n",
      "+------------------------------------------------------+\n",
      "| Top Contributions               | Contribution Value |\n",
      "+---------------------------------+--------------------+\n",
      "| sepalWidth                      |  4.936926469035047 |\n",
      "| petalWidth                      |  3.436492303761425 |\n",
      "| sepalLength                     |  1.559475987118261 |\n",
      "| petalLength                     | 0.8951550768404886 |\n",
      "| sepalWidth(sepalWidth = null)   |                0.0 |\n",
      "| sepalLength(sepalLength = null) |                0.0 |\n",
      "| petalLength(petalLength = null) |                0.0 |\n",
      "| petalWidth(petalWidth = null)   |                0.0 |\n",
      "+---------------------------------+--------------------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mfittedWorkflow\u001b[39m: \u001b[32mOpWorkflowModel\u001b[39m = com.salesforce.op.OpWorkflowModel@75c70b2b"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val fittedWorkflow = workflow.train()\n",
    "println(\"Summary:\\n\" + fittedWorkflow.summaryPretty())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After model has been fitted we use scoreAndEvaluate() function to evaluate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scoring the model:\n",
      "=================\n",
      "Transformed dataframe columns:\n",
      "--------------------------\n",
      "key\n",
      "irisClass_1-stagesApplied_RealNN_000000000006\n",
      "irisClass-petalLength-petalWidth-sepalLength-sepalWidth_4-stagesApplied_Prediction_000000000011\n",
      "Metrics:\n",
      "------------\n",
      "{\n",
      "  \"Precision\" : 0.9604700854700854,\n",
      "  \"Recall\" : 0.96,\n",
      "  \"F1\" : 0.9602349852021629,\n",
      "  \"Error\" : 0.040000000000000036,\n",
      "  \"ThresholdMetrics\" : {\n",
      "    \"topNs\" : [ 1, 3 ],\n",
      "    \"thresholds\" : [ 0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.2, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.3, 0.31, 0.32, 0.33, 0.34, 0.35, 0.36, 0.37, 0.38, 0.39, 0.4, 0.41, 0.42, 0.43, 0.44, 0.45, 0.46, 0.47, 0.48, 0.49, 0.5, 0.51, 0.52, 0.53, 0.54, 0.55, 0.56, 0.57, 0.58, 0.59, 0.6, 0.61, 0.62, 0.63, 0.64, 0.65, 0.66, 0.67, 0.68, 0.69, 0.7, 0.71, 0.72, 0.73, 0.74, 0.75, 0.76, 0.77, 0.78, 0.79, 0.8, 0.81, 0.82, 0.83, 0.84, 0.85, 0.86, 0.87, 0.88, 0.89, 0.9, 0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0 ],\n",
      "    \"correctCounts\" : {\n",
      "      \"1\" : [ 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 144, 143, 142, 141, 140, 140, 138, 137, 137, 137, 136, 135, 134, 134, 132, 130, 128, 122, 117, 117, 115, 114, 111, 109, 107, 104, 103, 98, 94, 90, 84, 80, 79, 76, 71, 70, 68, 67, 64, 62, 58, 56, 55, 52, 51, 51, 50, 48, 45, 35, 0 ],\n",
      "      \"3\" : [ 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 149, 149, 148, 148, 148, 148, 147, 147, 147, 147, 146, 146, 146, 145, 144, 143, 142, 141, 140, 140, 138, 137, 137, 137, 136, 135, 134, 134, 132, 130, 128, 122, 117, 117, 115, 114, 111, 109, 107, 104, 103, 98, 94, 90, 84, 80, 79, 76, 71, 70, 68, 67, 64, 62, 58, 56, 55, 52, 51, 51, 50, 48, 45, 35, 0 ]\n",
      "    },\n",
      "    \"incorrectCounts\" : {\n",
      "      \"1\" : [ 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 4, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ],\n",
      "      \"3\" : [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 6, 4, 4, 4, 4, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 ]\n",
      "    },\n",
      "    \"noPredictionCounts\" : {\n",
      "      \"1\" : [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 5, 6, 7, 9, 10, 10, 11, 12, 13, 14, 15, 17, 20, 22, 28, 33, 33, 35, 36, 39, 41, 43, 46, 47, 52, 56, 60, 66, 70, 71, 74, 79, 80, 82, 83, 86, 88, 92, 94, 95, 98, 99, 99, 100, 102, 105, 115, 150 ],\n",
      "      \"3\" : [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 4, 5, 6, 7, 9, 10, 10, 11, 12, 13, 14, 15, 17, 20, 22, 28, 33, 33, 35, 36, 39, 41, 43, 46, 47, 52, 56, 60, 66, 70, 71, 74, 79, 80, 82, 83, 86, 88, 92, 94, 95, 98, 99, 99, 100, 102, 105, 115, 150 ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdataframe\u001b[39m: \u001b[32mDataFrame\u001b[39m = [key: string, irisClass_1-stagesApplied_RealNN_000000000006: double ... 1 more field]\n",
       "\u001b[36mmetrics\u001b[39m: \u001b[32mevaluators\u001b[39m.\u001b[32mEvaluationMetrics\u001b[39m = \u001b[33mMultiClassificationMetrics\u001b[39m(\n",
       "  \u001b[32m0.9604700854700854\u001b[39m,\n",
       "  \u001b[32m0.96\u001b[39m,\n",
       "  \u001b[32m0.9602349852021629\u001b[39m,\n",
       "  \u001b[32m0.040000000000000036\u001b[39m,\n",
       "  \u001b[33mThresholdMetrics\u001b[39m(\n",
       "    \u001b[33mWrappedArray\u001b[39m(\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m),\n",
       "    \u001b[33mWrappedArray\u001b[39m(\n",
       "      \u001b[32m0.0\u001b[39m,\n",
       "      \u001b[32m0.01\u001b[39m,\n",
       "      \u001b[32m0.02\u001b[39m,\n",
       "      \u001b[32m0.03\u001b[39m,\n",
       "      \u001b[32m0.04\u001b[39m,\n",
       "      \u001b[32m0.05\u001b[39m,\n",
       "..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println(\"Scoring the model:\\n=================\")\n",
    "val (dataframe, metrics) = fittedWorkflow.scoreAndEvaluate(evaluator = evaluator)\n",
    "\n",
    "println(\"Transformed dataframe columns:\\n--------------------------\")\n",
    "dataframe.columns.foreach(println)\n",
    "\n",
    "println(\"Metrics:\\n------------\")\n",
    "println(metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala (2.11)",
   "language": "scala",
   "name": "scala211"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
